{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This is my respository of Neural Network functions that are helpful\n",
        "\n",
        "These are going to be the most useful for CNN or Linear Regression as that is all I have learned so far"
      ],
      "metadata": {
        "id": "4j_zKEXRz-oB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "w9E_aLO30dEq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import_prep_image\n",
        "\n",
        "Imports an image and resizes it to be used. If scale is false, image is not normalized."
      ],
      "metadata": {
        "id": "-kQ6VWKF0gRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "\n",
        "def import_prep_image(filename, img_shape=224, scale = True):\n",
        "  \"\"\"\n",
        "  Imports an image from a filename, turns it into a tensor and reshapes into\n",
        "  (img_shape, img_shape, 3).\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  filename (str): filename of image\n",
        "  img_shape (int): size to resize to, default 224\n",
        "  scale (bool): whether to scale or not\n",
        "  \"\"\"\n",
        "\n",
        "  # Read in image\n",
        "  image = tf.io.read_file(filename)\n",
        "  # Turn into tensor\n",
        "  image = tf.image.decode_jpeg(image)\n",
        "  # Resize\n",
        "  image = tf.image.resize(image, [img_shape, img_shape])\n",
        "\n",
        "  # Normalize if wanted\n",
        "  if scale:\n",
        "    image = image/255.\n",
        "  else:\n",
        "    return image"
      ],
      "metadata": {
        "id": "6KqZvbiT0tjn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pred_plus_plot\n",
        "\n",
        "Predict + Plot it"
      ],
      "metadata": {
        "id": "04QRwxdd8scT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pred_plus_plot(model, image_path, class_names, show_axis=False):\n",
        "    \"\"\"\n",
        "    Predicts the class of an image using a trained model and plots the image with the predicted class as the title.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: Trained model used for making predictions.\n",
        "    image_path: Path to the target image.\n",
        "    class_names: List of class names corresponding to the model's outputs.\n",
        "    show_axis: Whether to display the plot axis (default=False).\n",
        "\n",
        "    \"\"\"\n",
        "    # Load and preprocess the image\n",
        "    img = import_prep_image(image_path)\n",
        "\n",
        "    # Make a prediction\n",
        "    prediction = model.predict(tf.expand_dims(img, axis=0))\n",
        "\n",
        "    # Determine the predicted class\n",
        "    if len(prediction[0]) > 1:  # For multi-class classification\n",
        "        predicted_class = class_names[prediction.argmax()]\n",
        "    else:  # For binary classification\n",
        "        predicted_class = class_names[int(tf.round(prediction)[0][0])]\n",
        "\n",
        "    # Plot the image with the predicted class\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Prediction: {predicted_class}\")\n",
        "    if not show_axis:\n",
        "        plt.axis(False)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ey-PqKk89JQL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## make_callback_TB\n",
        "\n",
        "Makes tensorboard callback with files saved to \"log_dir/experiment_name/current_datetime/\""
      ],
      "metadata": {
        "id": "7me3jUY79z3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "def make_callback_TB(log_dir, experiment_name):\n",
        "    \"\"\"\n",
        "    Creates a TensorBoard callback to store log files.\n",
        "\n",
        "    Log files are saved to:\n",
        "        \"log_dir/experiment_name/current_datetime/\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    log_dir: Directory to store TensorBoard log files.\n",
        "    experiment_name: Name of the experiment directory (e.g., \"model_1\").\n",
        "    \"\"\"\n",
        "    # Define the directory for log files\n",
        "    save_path = f\"{log_dir}/{experiment_name}/{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
        "\n",
        "    # Create the TensorBoard callback\n",
        "    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=save_path)\n",
        "\n",
        "    print(f\"Saving TensorBoard log files to: {save_path}\")\n",
        "    return tb_callback\n"
      ],
      "metadata": {
        "id": "ByA0BJMn-CZj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## unzip_folder\n",
        "\n",
        "Basically just unzips a folder"
      ],
      "metadata": {
        "id": "SgL_808__j16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "def unzip_folder(zip_path):\n",
        "    \"\"\"\n",
        "    Unzips a folder\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    zip_path: str, path to file\n",
        "    \"\"\"\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n"
      ],
      "metadata": {
        "id": "6CWUtLQE_0W0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## compare_historys\n",
        "\n",
        "Takes in the original history and new history and makes a graph comparing them"
      ],
      "metadata": {
        "id": "CO7hj_mw_Rcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_historys(original_history, new_history, initial_epochs=5):\n",
        "    \"\"\"\n",
        "    Compares two TensorFlow model History objects.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    original_history: History object from the initial model training.\n",
        "    new_history: History object from the continued model training.\n",
        "    initial_epochs: Number of epochs in original_history (new_history plot starts from this point).\n",
        "    \"\"\"\n",
        "    # Extract metrics from original history\n",
        "    train_acc = original_history.history[\"accuracy\"]\n",
        "    train_loss = original_history.history[\"loss\"]\n",
        "    val_acc = original_history.history[\"val_accuracy\"]\n",
        "    val_loss = original_history.history[\"val_loss\"]\n",
        "\n",
        "    # Combine original history with new history\n",
        "    total_train_acc = train_acc + new_history.history[\"accuracy\"]\n",
        "    total_train_loss = train_loss + new_history.history[\"loss\"]\n",
        "    total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n",
        "    total_val_loss = val_loss + new_history.history[\"val_loss\"]\n",
        "\n",
        "    # Create plots\n",
        "    plt.figure(figsize=(8, 8))\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(total_train_acc, label='Training Accuracy')\n",
        "    plt.plot(total_val_acc, label='Validation Accuracy')\n",
        "    plt.axvline(x=initial_epochs - 1, linestyle='--', color='gray', label='Start Fine Tuning')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(total_train_loss, label='Training Loss')\n",
        "    plt.plot(total_val_loss, label='Validation Loss')\n",
        "    plt.axvline(x=initial_epochs - 1, linestyle='--', color='gray', label='Start Fine Tuning')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "DlG8ncz5_gG_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loss_plot\n",
        "\n",
        "Makes two plots that compare the loss curves for the training and validation metrics."
      ],
      "metadata": {
        "id": "_C46LNCB-KGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def loss_plot(history):\n",
        "    \"\"\"\n",
        "    Plots loss and accuracy curves for training and validation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    history: TensorFlow model History object containing training metrics.\n",
        "    \"\"\"\n",
        "    # Extract metrics from history\n",
        "    train_loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    train_accuracy = history.history['accuracy']\n",
        "    val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "    epochs = range(len(train_loss))  # Number of epochs\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.plot(epochs, train_loss, label='Training Loss')\n",
        "    plt.plot(epochs, val_loss, label='Validation Loss')\n",
        "    plt.title('Loss Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training and validation accuracy\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, train_accuracy, label='Training Accuracy')\n",
        "    plt.plot(epochs, val_accuracy, label='Validation Accuracy')\n",
        "    plt.title('Accuracy Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "u5S9aiwh-nmK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## break_down_dir\n",
        "\n",
        "Breaks down a directory listing all of the contents"
      ],
      "metadata": {
        "id": "SeiJIdjPAPCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def break_down_dir(directory):\n",
        "    \"\"\"\n",
        "    Analyzes the contents of a directory, listing subdirectories and file counts.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    directory: str, path to the directory to be analyzed\n",
        "\n",
        "    Outputs\n",
        "    -------\n",
        "    Prints the number of subdirectories, the number of files in each subdirectory,\n",
        "    and the name of each subdirectory.\n",
        "    \"\"\"\n",
        "    for dirpath, dirnames, filenames in os.walk(directory):\n",
        "        print(f\"There are {len(dirnames)} subdirectories and {len(filenames)} files in '{dirpath}'.\")\n"
      ],
      "metadata": {
        "id": "dI_XHgOBAkbL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create_confusion_matrix\n",
        "\n",
        "Creates a confusion matrix based on user needs. Matrix is a remix of Scikit-Learn's example"
      ],
      "metadata": {
        "id": "WUZf91op15sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "def create_confusion_matrix(true_labels, pred_labels, class_names=None, size=(10, 10), font_size=15, normalize=False, save_image=False, x_rotate=False):\n",
        "    \"\"\"\n",
        "    Creates a confusion matrix comparing actual labels and predicted labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    true_labels: Array of actual labels (same shape as pred_labels).\n",
        "    pred_labels: Array of predicted labels (same shape as true_labels).\n",
        "    class_names: List of class names (optional). If None, integer labels are used.\n",
        "    size: Tuple indicating the size of the plot (default=(10, 10)).\n",
        "    font_size: Font size for text in the plot (default=15).\n",
        "    normalize: Whether to show percentages instead of raw counts (default=False).\n",
        "    save_image: Whether to save the confusion matrix as an image (default=False).\n",
        "    x_rotate: Whether to rotate x-axis labels for better readability (default=False), make True for models with many classes\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A confusion matrix plot comparing true_labels and pred_labels.\n",
        "\n",
        "    Example:\n",
        "        create_confusion_matrix(\n",
        "            true_labels=test_labels,\n",
        "            pred_labels=test_preds,\n",
        "            class_names=label_names,\n",
        "            size=(12, 12),\n",
        "            font_size=12,\n",
        "            x_rotate=True\n",
        "        )\n",
        "    \"\"\"\n",
        "    # Generate the confusion matrix\n",
        "    cm = confusion_matrix(true_labels, pred_labels)\n",
        "    cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]  # Normalize if needed\n",
        "    num_classes = cm.shape[0]  # Total number of classes\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    fig, ax = plt.subplots(figsize=size)\n",
        "    color_map = ax.matshow(cm, cmap=plt.cm.Blues)\n",
        "    fig.colorbar(color_map)\n",
        "\n",
        "    # Set up labels\n",
        "    labels = class_names if class_names else np.arange(num_classes)\n",
        "\n",
        "    # Add axis titles and tick labels\n",
        "    ax.set(title=\"Confusion Matrix\",\n",
        "           xlabel=\"Predicted\",\n",
        "           ylabel=\"Actual\",\n",
        "           xticks=np.arange(num_classes),\n",
        "           yticks=np.arange(num_classes),\n",
        "           xticklabels=labels,\n",
        "           yticklabels=labels)\n",
        "\n",
        "    # Position x-axis labels at the bottom\n",
        "    ax.xaxis.set_label_position(\"bottom\")\n",
        "    ax.xaxis.tick_bottom()\n",
        "\n",
        "    # Rotate labels if x_rotate is True\n",
        "    if x_rotate:\n",
        "        plt.xticks(rotation=70, fontsize=font_size)\n",
        "    plt.yticks(fontsize=font_size)\n",
        "\n",
        "    # Determine threshold for text color\n",
        "    threshold = (cm.max() + cm.min()) / 2.\n",
        "\n",
        "    # Add text to each cell\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, f\"{cm[i, j]} ({cm_normalized[i, j]*100:.1f}%)\",\n",
        "                     ha=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "                     fontsize=font_size)\n",
        "        else:\n",
        "            plt.text(j, i, f\"{cm[i, j]}\",\n",
        "                     ha=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "                     fontsize=font_size)\n",
        "\n",
        "    # Save the confusion matrix as an image if requested\n",
        "    if save_image:\n",
        "        fig.savefig(\"confusion_matrix.png\")\n"
      ],
      "metadata": {
        "id": "qnsImVbx8Kg5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## find_most_wrong\n",
        "\n",
        "Function that makes a pandas dataframe to find the most wrong predictions"
      ],
      "metadata": {
        "id": "LeYX3QC0LdrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def find_most_wrong(model, filepath_dataset, test_data, num_wrong):\n",
        "\n",
        "  class_names = test_data.class_names\n",
        "\n",
        "  y_labels = []\n",
        "  for images, labels in test_data.unbatch():\n",
        "    y_labels.append(labels.numpy().argmax())\n",
        "\n",
        "  # make predictions\n",
        "  preds_probs = model.predict(test_data, verbose = 1)\n",
        "\n",
        "  # get all image file paths\n",
        "  filepaths = []\n",
        "  for filepath in test_data.list_files(filepath_dataset + \"/*/*.jpg\",\n",
        "                                      shuffle = False):\n",
        "    filepaths.append(filepath.numpy())\n",
        "\n",
        "  # Make dataframe\n",
        "\n",
        "  pred_df = pd.DataFrame({\"img_path\": filepaths,\n",
        "                        \"y_true\": y_labels,\n",
        "                        \"y_pred\": pred_classes,\n",
        "                        \"pred_conf\": preds_probs.max(axis = 1), # get max  prediction probability value\n",
        "                        \"y_true_classname\": [class_names[i] for i in y_labels],\n",
        "                        \"y_pred_classname\": [class_names[i] for i in pred_classes]})\n",
        "\n",
        "  # Find what is wrong\n",
        "  pred_df[\"pred_correct\"] = pred_df[\"y_true\"] == pred_df[\"y_pred\"]\n",
        "\n",
        "  # Gets specified amount of top wrong\n",
        "  top_wrong = pred_df[pred_df[\"pred_correct\"] == False].sort_values(\"pred_conf\", ascending = False)[:num_wrong]\n",
        "  # Displays them\n",
        "  top_wrong.head(num_wrong)\n",
        "\n",
        "  return top_wrong"
      ],
      "metadata": {
        "id": "rxhHfeJkLaSG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}